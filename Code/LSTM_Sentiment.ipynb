{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7q8n6_Xgpj_Y"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj354J7eq2g2",
        "outputId": "c1a5de79-26ad-4004-db32-3c566c7d9ed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Berfungsi untuk mengunduh leksikon VADER (**Valence Aware Dictionary and Sentiment Reasoner**) dari pustaka NLTK (**Natural Language Toolkit**). Digunakan dalam analisis sentiment texks."
      ],
      "metadata": {
        "id": "u-ljS7E6CjOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"Wuthering_Reviews.csv\")"
      ],
      "metadata": {
        "id": "feJuzbuorAcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Berfungsi untuk membaca file CSV bernama `Wuthering_Reviews.csv` dan membuat isi kedalam DataFrame bernama df menggunakan pustaka pandas (`pd`)."
      ],
      "metadata": {
        "id": "AWV81J4RCy4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Review'] = df['Review'].astype(str)"
      ],
      "metadata": {
        "id": "-yVb4nd6tJO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Berfungsi untuk mengonversi seluruh nilai dalam kolom `Review` menjadi tipe data string (`str`), lalu menyimpan kembali ke dalam kolom yang sama dalam DataFrame `df`"
      ],
      "metadata": {
        "id": "QyvvZhLqC-7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sia = SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "id": "TrZaftmIu4Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Berfungsi untuk membuat sebuah objek `SentimentIntensityAnalyzer` dari pustaka `nltk.sentiment.vader`. Objek ini digunakan untuk menganalisis sentimen dari teks, seperti kalimat atau ulasan."
      ],
      "metadata": {
        "id": "YAm4Hn5-DU-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentiment(text):\n",
        "    score = sia.polarity_scores(text)\n",
        "    if score['compound'] >= 0.05:\n",
        "        return 1\n",
        "    elif score['compound'] <= -0.05:\n",
        "        return 0\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "IUclFdlBunsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bertujuan untuk mengklasifikasikan sentimen dari suatu teks sebagai **positif(1)** dan **Negatif(0)** berdasarkan skor compound dari VADER.\n",
        "\n",
        "Penjelasan:\n",
        "1. `score = sia.polarity_scores(text)`\n",
        "Mengambil skor sentiment dari teks menggunakan VADER. Skor ini berbentuk dictionary seperti ini:\n",
        "\n",
        "\n",
        "```\n",
        "{'neg': 0.1, 'neu': 0.7, 'pos': 0.2, 'compound': 0.3612}\n",
        "```\n",
        "\n",
        "2. `if score['compound'] >= 0.05:`\n",
        "Jika skor gabungan (compound) cukup tinggi -> dianggap **POSITIF** -> return `1`\n",
        "\n",
        "3. `elif score['compound'] <= -0.05:`\n",
        "Jika skor compound cukup rendah -> dianggap **NEGATIF** -> return `0`.\n",
        "\n",
        "4. else:\n",
        "Jika skor berada di antara -0.05 dan 0.05 -> dianggap **NETRAL** -> return `None`\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y9QDIQfpDimk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Sentiment'] = df['Review'].apply(get_sentiment)\n",
        "df = df.dropna(subset=['Sentiment'])"
      ],
      "metadata": {
        "id": "9NoNkY-MuwUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Digunakan untuk mengklasifikasikan sentimen setiap ulasan dan menghapus baris yang tidak memilki sentimen yang jelas (netral) dari DataFrame\n",
        "\n",
        "Penjelasan:\n",
        "1.  `df['Sentiment'] = df['Review'].apply(get_sentiment)`\n",
        "- Menerapkan fungsi `get_sentiment` (yang telah dibuat sebelumnya) ke setiap elemen dalam kolom `Review`.\n",
        "- Hasil klasifikasi (0 = negatif, 1 = positif, None = netral) disimpan di kolom baru bernama `Sentiment`"
      ],
      "metadata": {
        "id": "UpEozOicEgq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "  text = str(text).lower()\n",
        "  text = re.sub(r\"http\\S+|www\\S+|@\\w+|#\", \"\", text)\n",
        "  text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "  text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "  return text\n",
        "\n",
        "df['Clean_Reviews'] = df ['Review'].apply(clean_text)"
      ],
      "metadata": {
        "id": "4lXXTzaIvOkk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de2afc7e-35ec-405f-b370-77785602e3c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-2f9fd4e0897d>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['Clean_Reviews'] = df ['Review'].apply(clean_text)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Digunakan untuk membersihkan teks ulasan sebelum diproses lebih lanjut dalam pipeline NLP.\n",
        "\n",
        "Penjelasan:\n",
        "1. `text = str(text).lower()`\n",
        "Mengonversi teks menjadi string (untuk jaga jaga jika ada nilai non string) dan semua huruf ke huruf kecil, agar standar.\n",
        "\n",
        "2. `text = re.sub(r\"http\\S+|www\\S+|@\\w+|#\", \"\", text)`\n",
        "Menghapus tautan (URL), mention (@username), dan simbol tagar (#).\n",
        "\n",
        "3. `text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)`\n",
        "Menghapus tanda baca dan karakter non-alfanumerik, menyisakan hanya huruf, angka, dan spasi.\n",
        "\n",
        "4. `text = re.sub(r\"\\s+\", \" \", text).strip()`\n",
        "Menghapus spasi berlebih (termasuk tab dan newline), dan menghilangkan spasi diawal/akhir teks"
      ],
      "metadata": {
        "id": "vBR4xQaTFAe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(df['Clean_Reviews'])"
      ],
      "metadata": {
        "id": "6dK8AzotvfSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Digunakan untuk mempersiapkan data teks agar bisa diubah menjadi urutan angka yang dapat diproses oleh model machine learning.\n",
        "\n",
        "Penjelasan:\n",
        "1. `tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')`\n",
        "`Tokenizer` adalah kelas dari `tensorflow.keras.preprocessing.text` yang digunakan untuk:\n",
        "- Membuat indeks kata berdasarkan frekuensi kemunculan.\n",
        "- mengubah teks menjadi urutan angka (integer sequences).\n",
        "\n",
        "2. `num_words=10000`:\n",
        "- Hanya **10.000 kata teratas** yang paling sering muncul yang akan dipertahankan dalam kamustokenizer.\n",
        "- kata diluar daftar ini akan dianggap sebagai kata yang jarang (rare words).\n",
        "\n",
        "3. `oov_token='<OOV>'`:\n",
        "- Singkatan dari *Out Of Vocabulary*.\n",
        "- Kata - kata yang tidak ditemukan selama `fit` akan digantikan dengan token khusus saat digunakan nanti `(text_to_sequences())`.\n",
        "- Membantu menjaga model tetap stabils aat menerima kata baru di data uji / prediksi."
      ],
      "metadata": {
        "id": "AMfCjcE4FqTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = tokenizer.texts_to_sequences(df['Clean_Reviews'])\n",
        "X = pad_sequences(X, maxlen=100)"
      ],
      "metadata": {
        "id": "glw76nChvqBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Digunakan untuk mengonversi teks menjadi urutan angka dan memastikan semua urutan memiliki panjang yang sama agr bisa diproses oleh model.\n",
        "\n",
        "Penjelasan:\n",
        "1. `X = tokenizer.texts_to_sequences(df['Clean_Reviews'])`\n",
        "- Mengubah setiap review bersih (`Clean_Reviews`) menjadi daftar angka.\n",
        "- Angka tersebut adalah indeks kata berdasarkan kamus/tokenizer yang sudah dibuat sebelumnya.\n",
        "- Kata-kata yang tidak dikenali (tidak masuk 10.000 kata teratas) akan diganti dengan indeks dari `<OOV>` token.\n",
        "contoh:\n",
        "\n",
        "\n",
        "```\n",
        "[\"this book is great\"] → [[12, 345, 6, 789]]\n",
        "```\n",
        "\n",
        "2. X = pad_sequences(X, maxlen=100)\n",
        "- Mengubah semua urutan angka menjadi panjang tetap (dalam hal ini, 100 token).\n",
        "- jika suatu runtutan lebih pendek dari 100, maka akan **ditambahkan padding (nol)** didepan (`default: padding='pre`).\n",
        "- jika lebih panjang dari 100, maka urutan akan dipotong dari awal(`default: trauncating='pre'`)"
      ],
      "metadata": {
        "id": "8OKjFn_CGpJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['Sentiment'].astype(int)"
      ],
      "metadata": {
        "id": "OVJG0ZaXvtZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Berfungsi untuk mengonversi kolom Sentiment menjadi tipe data integer (1 untuk positif dan 0 untuk negatif) yangdigunakan sebagai label target dalam model pembelajaran mesin.\n",
        "\n",
        "Penjelasan:\n",
        "1. `df['Sentiment']`: Kolom `Sentiment` yang sebelumnya berisi nilai klasifikasi sentimen (`1`, `0`, atau `None`).\n",
        "2. `.astype(int)`: Mengonversi nilai dalam kolom `Sentiment` menjadi tipe integer.\n",
        "\n",
        "- Positif (`1`).\n",
        "\n",
        "- Negatif (`0`)."
      ],
      "metadata": {
        "id": "UcwCFRWHIJPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Rz53R0E6v5bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Digunakan untuk membagi data menjadi set pelatihan dan set pengujian.\n",
        "\n",
        "Penjelasan:\n",
        "1. `train_test_split` adalah fungsi dari pustaka `sklearn.model_selection` yang digunakan untuk membagi data menjadi dua bagian:\n",
        "- Set pelatihan (`X_train`, `y_train`): Digunakan untuk melatih model.\n",
        "- Set penguji (`X_test`, `y_test`): Digunakan untuk mengevaluasi kinerja model setelah pelatihan."
      ],
      "metadata": {
        "id": "oIP79lD6Itd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Embedding(10000, 64, input_length=100),\n",
        "    LSTM(64),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07JDiviUwDQC",
        "outputId": "c2ccb97f-598b-43a7-c5f0-9cbb520e064c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Digunakan untuk membangun model jaringan saraf (neural network) dengan menggunakan keras (bagian dari Tensorflow) untuk klasifikasi sentiment.\n",
        "\n",
        "Penjelasan:\n",
        "1. `Embedding(10000, 64, input_length=100)`\n",
        "- Tujuan: mengubah urutan angka (token yang dihasilkan sebelumnya) menjadi vektor kata berdimensi lebih rendah.\n",
        "- `10000`: ukuran maksimum kata yang akan dipertimbangkan oleh model (berarti hanya 10.000 kata teratas yang akan diproses).\n",
        "- `64`: Dimensi ruang vektor untuk setiap kata (setiap kata akandipetakan menjadi vektor dengan panjang 64).\n",
        "- `input_length=100`: panjang input untuks etiap contoh (dalam hal ini, 100 token). Artinya setiap input harus memiliki panjang 100 (padded sequences).\n",
        "2. `LSTM(64)`\n",
        "- Tujuan: menggunakan **Long Short-Term Memory (LSTM)**, jenis **Recurrent Neural Network (RNN)**, untuk menangkap informasi kontekstual dalam urutan data(seperti hubungan antara kata-kata dalam kalimat).\n",
        "- `64`: Jumlah untuk dalam layer LSTM, yang menentukan jumlah neuron yang akan digunakan dalam lapisan LSTM.\n",
        "3. `Dropout(0.5)`\n",
        "- Tujuan: Dropout digunakan untuk mengurangi overfitting dengan \"menonaktifkan\" sebagaian neuron selama pelatihan secara acak.\n",
        "- `0.5`: artinya 50% neuron akan dimatikan selama pelatihan untuk mencegah model terlalu mengandalkan fitur-fitur tertentu yang mungkin hanya berlaku untuk data pelatihan saja.\n",
        "4. `Dense(1, activation='sigmoid')`\n",
        "- Tujuan: Lapisan terakhir dari model.\n",
        "- `1`: model hanya menghasilkan satu output (karena kita hanya memiliki dua kelas: positif dan negatif).\n",
        "- `activation='sigmoid'`: Fungsi aktivitas sigmoid menghasilkan output antara 0 dan 1, yang cocok untuk masalah klasifikasi biner, jika outout lebih besar dari 0.5, itu berarti sentimen positif (1), jika kurang dari 0.5, itu berarti sentimen negatif (0)."
      ],
      "metadata": {
        "id": "sBmigo0rJF9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "sLy6CEFRwMUT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "dfcf895e-e7da-4b22-b258-44df1b0b268b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Berfungsi untuk mengkompilasi model dan menampilkan ringkasan arsitektur model.\n",
        "\n",
        "Penjelasan:\n",
        "1. `model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])`\n",
        "- `loss='binary_crossentropy'`: Fungsi kerugian yang digunakan untuk klasifikasi biner (positif/negatif).\n",
        "- `optimizer='adam'`: Optimizer yang digunakan untuk mengupdate bobot model selama pelatihan.\n",
        "- `metrics=['accuracy']`: Menghitung akurasi sebagai metrik evaluasi selama pelatihan dan pengujian.\n",
        "\n",
        "2. `model.summary()`\n",
        "- Tujuan: Menampilkan ringkasan arsitektur model.\n"
      ],
      "metadata": {
        "id": "FcDeXTS3KmER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2IE1mjXwPad",
        "outputId": "9f905e6a-d619-46ce-8685-b64e987d4123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 67ms/step - accuracy: 0.7943 - loss: 0.5419 - val_accuracy: 0.8141 - val_loss: 0.4377\n",
            "Epoch 2/5\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - accuracy: 0.8363 - loss: 0.3620 - val_accuracy: 0.8694 - val_loss: 0.3218\n",
            "Epoch 3/5\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - accuracy: 0.9202 - loss: 0.2297 - val_accuracy: 0.8776 - val_loss: 0.3034\n",
            "Epoch 4/5\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - accuracy: 0.9714 - loss: 0.1130 - val_accuracy: 0.8859 - val_loss: 0.3407\n",
            "Epoch 5/5\n",
            "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 70ms/step - accuracy: 0.9842 - loss: 0.0702 - val_accuracy: 0.8835 - val_loss: 0.3558\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x796c2d409350>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Digunakan untuk melatih model menggunakan data pelatihan (`X_train` dan `y_train`) dan mengevaluasi kinerjanya dengan data pengujian (`X_test` dan `y_test`) selama pelatihan.\n",
        "\n",
        "Penjelasan:\n",
        "1. `X_train` dan `y_train`:\n",
        "- `X_train`: Data fitur untuk pelatihan (urutan angka dari teks yang sudah diproses).\n",
        "- `y_train`: Label target untuk pelatihan (sentimen positif atau negatif).\n",
        "\n",
        "2. `epochs=5`:\n",
        "- Menentukan jumlah iterasi untuk pelatihan model di seluruh data.\n",
        "\n",
        "3. `batch_size=32`:\n",
        "- Batch size adalah jumlah sampel yang diproses sebelum model melakukan pembaruan bobot.\n",
        "\n"
      ],
      "metadata": {
        "id": "0j5l2wlkLAoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = (model.predict(X_test) >= 0.5).astype(int)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7dEFqYZkUpp",
        "outputId": "dc55ef39-0154-430c-8fbc-ac6703c5c2c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.64      0.67       158\n",
            "           1       0.92      0.94      0.93       692\n",
            "\n",
            "    accuracy                           0.88       850\n",
            "   macro avg       0.81      0.79      0.80       850\n",
            "weighted avg       0.88      0.88      0.88       850\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Digunakan untuk melakukan prediksi pada data pengujian dan kemudian menampilkan laporan klasifikasi untuk mengevaluasi kinerja model."
      ],
      "metadata": {
        "id": "C3txzx3oL40h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Predicted_Sentiment\"] = (model.predict(X) >= 0.5).astype(int)\n",
        "df[[\"Review\", \"Clean_Reviews\", \"Predicted_Sentiment\"]].to_csv(\"Sentiment_Wuthering.csv\", index=False)\n",
        "print(\"✅ Sentiment_Game.csv berhasil disimpan.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebjQZPp-k2yQ",
        "outputId": "653a9de2-712e-4e29-85a5-2d29631d4de7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
            "✅ Sentiment_Game.csv berhasil disimpan.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"sentiment_model.keras\")\n",
        "print(\"✅ Model berhasil disimpan.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSr5fquvk6HJ",
        "outputId": "f8867bea-ace1-4ea2-c56f-d95476686067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model berhasil disimpan.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "tokenizer_json = tokenizer.to_json()\n",
        "with open(\"tokenizer_lstm.json\", \"w\") as json_file:\n",
        "    json.dump(tokenizer_json, json_file)"
      ],
      "metadata": {
        "id": "E6DV8GXomilx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}